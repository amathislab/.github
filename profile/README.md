# Welcome to the A. Mathis Group at EPFL! 

Broadly speaking, we work at the intersection of computational neuroscience and machine learning, aka AI4(Neuro)Science. Ultimately, we are interested in reverse engineering the algorithms of the brain, in order to figure out how the brain works and to build better artificial intelligence systems.  

Check out [group's website for more information](http://www.mathisgroup.org), and see our open source code below!

## Packages for behavioral analysis:

- [DeepLabCut](https://github.com/DeepLabCut/DeepLabCut): for animal pose estimation
- [DLC2action](https://github.com/AlexEMG/DLC2action): for action segmentation 

## Selected Code from published research projects ðŸ‘©â€ðŸ’»:

**Computer Vision and Behavioral Analysis:**
- [WildCLIP: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models](https://github.com/amathislab/wildclip): Code for Gabeff, Russwurm, Tuia & Mathis. 
- [Bottom-up conditioned top-down pose estimation (BUCTD)](https://github.com/amathislab/BUCTD): Code for Zhou*, Stoffl*, Mathis and Mathis ICCV, 2023. State of the art code for performing 2D pose estimation in crowded scenes. 
- [End-to-end trainable multi-instance pose estimation with transformers](https://github.com/amathislab/poet): code for POET model, Stoffl, Vidal & Mathis arxiv 2021
- [AcinoSet: A 3D Pose Estimation Dataset and Baseline Models for Cheetahs in the Wild](https://github.com/amathislab/AcinoSet), ICRA 2021
- [Primer on Motion Capture](https://github.com/amathislab/Primer-MotionCapture), Neuron 2020

**Reinforcement learning:**
- [Latent Exploration for Reinforcement Learning](https://github.com/amathislab/lattice): Code for Chiappa, et al. NeurIPS 2023 
- [DMAP: a Distributed Morphological Attention Policy for Learning to Locomote with a Changing Body](https://github.com/amathislab/dmap): Code for Chiappa, Marin-Vargas & Mathis NeurIPS 2022
- [Winning code for the Baoding ball MyoChallenge at NeurIPS 2022](https://github.com/amathislab/myochallenge), joint work with Pouget Lab --> paper forthcoming.

**AI4Science and modeling sensorimotor control:**
- [ODEformer: symbolic regression of dynamical systems with transformers](https://github.com/sdascoli/odeformer): Code from d'Ascoli*, Becker*, Schwaller, Mathis & Kilbertus ICLR 2024 (spotlight). Cool code to infer symbolic formulas from data
- [DeepDraw](https://github.com/amathislab/DeepDraw): code for modeling proprioception with task-driven modeling, Sanbrink*, Mamidanna* et al. eLife 2023
- [Task-driven-proprioception](https://github.com/amathislab/Task-driven-Proprioception): code for modeling the proprioceptive system of primates. Marin Vargas* & Bisi* et al. BioRxiv 2023

ðŸŒˆ Please reach out, if you want to work with us! We love collaborative, open-source science.

<!--

**Here are some ideas to get you started:**

ðŸ™‹â€â™€ï¸ A short introduction - what is your organization all about?
ðŸŒˆ Contribution guidelines - how can the community get involved?
ðŸ‘©â€ðŸ’» Useful resources - where can the community find your docs? Is there anything else the community should know?
ðŸ¿ Fun facts - what does your team eat for breakfast?
ðŸ§™ Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
